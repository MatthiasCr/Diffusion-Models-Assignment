{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982819e3",
   "metadata": {},
   "source": [
    "# Denoising Probabilistic Diffusion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f1f66c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Colab-only setup\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Running in Google Colab. Setting up repo\")\n",
    "\n",
    "    !git clone https://github.com/MatthiasCr/Diffusion-Models-Assignment.git\n",
    "    %cd Diffusion-Models-Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb411b7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import clip\n",
    "import open_clip\n",
    "import wandb\n",
    "import urllib.request\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import UNet_utils, ddpm_utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a95276e",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a24916",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "T = 400\n",
    "B_start = 0.0001\n",
    "B_end = 0.02\n",
    "B = torch.linspace(B_start, B_end, T).to(device)\n",
    "\n",
    "IMG_SIZE = 32\n",
    "IMG_CH = 3\n",
    "BATCH_SIZE = 128\n",
    "INPUT_SIZE = (IMG_CH, IMG_SIZE, IMG_SIZE)\n",
    "GUIDANCE_WEIGHT = 2.0\n",
    "\n",
    "ddpm = ddpm_utils.DDPM(B, device)\n",
    "\n",
    "# Initialize the U-Net model \n",
    "model = UNet_utils.UNet(\n",
    "    T, img_ch=3, img_size=32, down_chs=(256, 256, 512), t_embed_dim=8, c_embed_dim=512\n",
    ").to(device)\n",
    "\n",
    "# load clip model to get text embeddings from prompts\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\")\n",
    "clip_model.eval()\n",
    "CLIP_FEATURES = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c7a2b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "weights_path = \"weights/model.pth\"\n",
    "url = \"https://github.com/MatthiasCr/Diffusion-Models-Assignment/releases/download/v1/model_weights.pth\"\n",
    "\n",
    "os.makedirs(\"weights\", exist_ok=True)\n",
    "\n",
    "if not os.path.exists(weights_path):\n",
    "    print(\"Downloading pretrained weights...\")\n",
    "    urllib.request.urlretrieve(url, weights_path)\n",
    "\n",
    "model.load_state_dict(torch.load(weights_path, map_location=device)) \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a03753",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# list of text prompts to generate images for.\n",
    "text_prompts = [\n",
    "    \"A photo of a red rose\",\n",
    "    \"An image of a red rose\",\n",
    "    \"A picture of a red rose\",\n",
    "    \"A red rose\",\n",
    "    \"A rose with red petals\"\n",
    "    \"A purple rose\",\n",
    "    \"A yellow rose\",\n",
    "    \"A blue rose\",\n",
    "\n",
    "    \"A photo of a white daisy\",\n",
    "    \"An image of a white daisy\",\n",
    "    \"A picture of a white daisy\",\n",
    "    \"A white daisy\",\n",
    "    \"A daisy that is white\",\n",
    "    \"A yellow daisy\",\n",
    "    \"A red daisy\",\n",
    "\n",
    "    \"A photo of a yellow sunflower\",\n",
    "    \"An image of a yellow sunflower\",\n",
    "    \"A picture of a yellow sunflower\",\n",
    "    \"A yellow sunflower\",\n",
    "    \"A sunflower\",\n",
    "    \"A sunflower with orange petals\"\n",
    "\n",
    "    \"An orange tulip\",\n",
    "    \"A rose tulip\",\n",
    "    \"A purple tulip\",\n",
    "    \"A photo of a white orchid\",\n",
    "    \"A rose orchid\",\n",
    "    \"A photo of a purple flower\",\n",
    "    \"A photo of a blue flower\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231c563",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Register a forward hook on the `down2` layer of the U-Net model.\n",
    "embeddings_storage = {}\n",
    "\n",
    "def get_embedding_hook(name):\n",
    "    def hook(model, input, output):\n",
    "        embeddings_storage[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model.down2.register_forward_hook(get_embedding_hook('down2'))\n",
    "\n",
    "# function to generate flower images from prompts\n",
    "def sample_flowers(text_list):\n",
    "    text_tokens = clip.tokenize(text_list).to(device)\n",
    "    c = clip_model.encode_text(text_tokens).float()\n",
    "    x_gen, x_gen_store = ddpm_utils.sample_w(model, ddpm, INPUT_SIZE, T, c, device, w_tests=[GUIDANCE_WEIGHT])\n",
    "    return x_gen, x_gen_store\n",
    "\n",
    "\n",
    "generated_images, _ = sample_flowers(text_prompts)\n",
    "extracted_embeddings = embeddings_storage['down2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1abe2d1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def show_tensor_image(image):\n",
    "    reverse_transforms = transforms.Compose([\n",
    "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "        transforms.Lambda(lambda t: torch.minimum(torch.tensor([1]), t)),\n",
    "        transforms.Lambda(lambda t: torch.maximum(torch.tensor([0]), t)),\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "    plt.imshow(reverse_transforms(image[0].detach().cpu()))\n",
    "\n",
    "grid = make_grid(generated_images.cpu())\n",
    "show_tensor_image([grid])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acbdde0",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd362b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_clip_score(image_path, text_prompt):\n",
    "    # Load model\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "    \n",
    "    # Preprocess inputs\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0)\n",
    "    tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "    text = tokenizer([text_prompt])\n",
    "\n",
    "    # Compute features and similarity\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text)\n",
    "        # Normalize features\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        # Calculate dot product\n",
    "        score = (image_features @ text_features.T).item()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb7ae3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_fid(real_embeddings, gen_embeddings):\n",
    "    # real_embeddings and gen_embeddings should be Numpy arrays of shape (N, 2048) \n",
    "    # extracted from an InceptionV3 model\n",
    "    # Calculate mean and covariance\n",
    "    mu1, sigma1 = real_embeddings.mean(axis=0), np.cov(real_embeddings, rowvar=False)\n",
    "    mu2, sigma2 = gen_embeddings.mean(axis=0), np.cov(gen_embeddings, rowvar=False)\n",
    "    # Calculate sum squared difference between means\n",
    "    ssdiff = np.sum((mu1 - mu2)**2)\n",
    "    # Calculate sqrt of product of covariances\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    # Handle numerical errors\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    # Final FID calculation\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4342be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import open_clip\n",
    "\n",
    "# TODO: Calculate the CLIP score for each generated image against its prompt.\n",
    "\n",
    "# You can use the `calculate_clip_score` function from the evaluation guide.\n",
    "\n",
    "# TODO: Calculate the FID score for the set of generated images.\n",
    "\n",
    "# You will need the `calculate_fid` function and the Inception model from the evaluation guide.\n",
    "\n",
    "# You will also need to load the real TF-Flowers dataset to compare against."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b215c22e",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9cdcd0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "n_generated_images = generated_images.shape[0]\n",
    "\n",
    "# Select the embeddings corresponding to the guided pass\n",
    "embeddings_guided_pass = extracted_embeddings[:n_generated_images]\n",
    "\n",
    "# Flatten the all dimensions except batch\n",
    "flattened_embeddings = embeddings_guided_pass.view(n_generated_images, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecdf276",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save generated images as .png\n",
    "images_dir = \"generated_images\"\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "image_filepaths = []\n",
    "for i, img_tensor in enumerate(generated_images):\n",
    "    img_filename = os.path.join(images_dir, f\"generated_image_{i}.png\")\n",
    "    save_image(img_tensor, img_filename)\n",
    "    image_filepaths.append(img_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cb54f8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset = fo.Dataset(name=\"generated_flowers_with_embeddings\")\n",
    "\n",
    "samples = []\n",
    "for i in range(n_generated_images):\n",
    "    sample = fo.Sample(filepath=image_filepaths[i])\n",
    "    sample[\"text_prompt\"] = fo.Classification(label=text_prompts[i])\n",
    "    sample[\"unet_embedding\"] = flattened_embeddings[i].tolist()\n",
    "    samples.append(sample)\n",
    "\n",
    "dataset.add_samples(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1640a4b1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Compute uniqueness and representativeness.\n",
    "fob.compute_uniqueness(dataset)\n",
    "fob.compute_representativeness(dataset, embeddings=\"unet_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d32aa4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset, auto=False)\n",
    "print(session.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fa239e",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08bc0e4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "\n",
    "run = wandb.init(project=\"diffusion_model_assessment\")\n",
    "\n",
    "# TODO: Log your hyperparameters (e.g., guidance weight `w`, number of steps `T`).\n",
    "\n",
    "# TODO: Log your evaluation metrics (CLIP Score and FID).\n",
    "\n",
    "# TODO: Create a wandb.Table to log your results. The table should include:\n",
    "\n",
    "# - The generated image.\n",
    "\n",
    "# - The text prompt.\n",
    "\n",
    "# - The CLIP score.\n",
    "\n",
    "# - The uniqueness score.\n",
    "\n",
    "# - The representativeness score.\n",
    "\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
